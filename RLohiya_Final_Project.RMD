---
title: "RLohiya Final Project"
author: "Ritesh Lohiya"
date: "December 7, 2018"
output: html_document
---

#                             FINAL PROJECT

#           IS 605 FUNDAMENTALS OF COMPUTATIONAL MATHEMATICS


```{r}
#Load libraries
suppressMessages(suppressWarnings(library(readr)))
suppressMessages(suppressWarnings(library(kableExtra)))
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(knitr)))
suppressMessages(suppressWarnings(library(psych)))
suppressMessages(suppressWarnings(library(gridExtra)))
suppressMessages(suppressWarnings(library(usdm)))
suppressMessages(suppressWarnings(library(mice)))
suppressMessages(suppressWarnings(library(ggiraph)))
suppressMessages(suppressWarnings(library(cowplot)))
suppressMessages(suppressWarnings(library(reshape2)))
suppressMessages(suppressWarnings(library(corrgram)))
suppressMessages(suppressWarnings(library(caTools)))
suppressMessages(suppressWarnings(library(caret)))
suppressMessages(suppressWarnings(library(ROCR)))
suppressMessages(suppressWarnings(library(pROC)))
suppressMessages(suppressWarnings(library(reshape2)))
suppressMessages(suppressWarnings(library(Amelia)))
suppressMessages(suppressWarnings(library(qqplotr)))
suppressMessages(suppressWarnings(library(moments)))
suppressMessages(suppressWarnings(library(car)))
suppressMessages(suppressWarnings(library(MASS)))
suppressMessages(suppressWarnings(library(geoR)))
suppressMessages(suppressWarnings(library(xtable)))
suppressMessages(suppressWarnings(library(plyr)))
suppressMessages(suppressWarnings(library(Hmisc)))
suppressMessages(suppressWarnings(library(corrplot)))
suppressMessages(suppressWarnings(library(PerformanceAnalytics)))
suppressMessages(suppressWarnings(library(ggpubr)))
suppressMessages(suppressWarnings(library(matrixcalc)))
suppressMessages(suppressWarnings(library(alr3)))
suppressMessages(suppressWarnings(library(bestglm)))
suppressMessages(suppressWarnings(library(car)))
suppressMessages(suppressWarnings(library(gridExtra)))
suppressMessages(suppressWarnings(library(scales)))
suppressMessages(suppressWarnings(library(Matrix)))
suppressMessages(suppressWarnings(library(Amelia)))
suppressMessages(suppressWarnings(library(mlr)))
suppressMessages(suppressWarnings(library(corrr)))
```

#Problem 1

##Pick one of the quantitative independent variables (Xi) from the data set below, and define that variable as  X.  Also, pick one of the dependent variables (Yi) below, and define that as Y.

```{r}

#download the files and then load them from storage
F1 <- read.csv("https://raw.githubusercontent.com/Riteshlohiya/Data605_Final_Project/master/F1.csv", stringsAsFactors = FALSE)
F1

#define X and Y.  
X <- F1$X1
Y <- F1$Y1
```


##Probability.

Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the 3d quartile of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable.  Interpret the meaning of all probabilities.

```{r}
# get quartiles
#"x" is 3d quartile of X variable
#"y" is 2d quartile of X variable
summary(X)
summary(Y)
```

The 3rd quartile of the X variable = 15.82 The 1st quartile of the Y variable = 18.55 So, x = 15.82 and y = 18.55

```{r}
x <- 15.82
y <- 18.55
```

```{r}
df<-data.frame(cbind(X,Y))
```

```{r}
PA_and_B <- nrow(subset(df, X > x & Y > y))/nrow(df)
PA <- nrow(subset(df, X > x))/nrow(df)
PB <- nrow(subset(df, Y > y))/nrow(df)
PC <- nrow(subset(df, X < x))/nrow(df)
PC_and_B <- nrow(subset(df, X < x & Y > y))/nrow(df)
```

## a. P(X>x | Y>y)

```{r}
# a. P(X>x | Y>y)
pA_given_B <- PA_and_B/PB
pA_given_B
```

P(X>x | Y>y) = .2 or 20%, which means that there is 20% probablity of X>x or X will be greater than than it 3rd quartile value of 15.82 given that the Y is greater than its 1st quartile value of 18.55.

## b. P(X>x, Y>y)

```{r}
# b. P(X>x, Y>y)
PA_and_B
```

P(X>x, Y>y) = .15 or 15%, which means that there is 15% probablity of X>x or X will be greater than than it 3rd quartile value of 15.82 while Y is greater than its 1st quartile value of 18.55.

## c. P(X < x | Y>y)

```{r}
# c. P(X<x|Y>y)
PC_given_B <- PC_and_B/PB
PC_given_B
```

P(X < x|Y>y) = .8 or 80%, which means that there is 80% probablity of X < x or X will be smaller than than it 3rd quartile value of 15.82 given that the Y is greater than its 1st quartile value of 18.55.



## Table of Counts


```{r}
data_tbl <- as.data.frame(cbind.data.frame(X, Y, t1 = ifelse(X > 
    x, ">1st quartile", "<=1st quartile"), Total = ifelse(Y > y, ">3d quartile", 
    "<=3d quartile")))

tbl <- addmargins(table(data_tbl$t1, data_tbl$Total, dnn = c("X/Y")))
tbl
```

##Does splitting the training data in this fashion make them independent? Let A be the new variable counting those observations above the 1st quartile for X, and let B be the new variable counting those observations above the 1st quartile for Y.    Does P(AB)=P(A)P(B)?   Check mathematically, and then evaluate by running a Chi Square test for association.

##Mathematical test:

```{r}
A <- tbl[2, 3]
A
```

```{r}
B <- tbl[3, 2]
B
```

```{r}
total <- tbl[3, 3]
total
```


```{r}
A_AND_B <- tbl[2, 2]

# P(A)
PA <- A/total

# P(B)
PB <- B/total

# P(A INT B)
PA_INT_B <- A_AND_B/total


# P(A|B) = P(A INT B) / P(B)

PA_GIVEN_B <- PA_INT_B/PB

# P(A) * P(B)

PA_INTO_PB <- (PA * PB)

PA_GIVEN_B

PA_INTO_PB

```

##Chi Square test

```{r}
chisq.test(X,Y) 
```

we can see that P(A|B) is 0.2 and P(A) * P(B) is 0.1875. They are very near. Therefore splitting the training data in this manner is going to make them independent. Also from the Chisq test we can see that P-Value is .22, so we cannot reject the null hypothesis.


#Problem 2

##You are to register for Kaggle.com (free) and compete in the House Prices: Advanced Regression Techniques competition.  https://www.kaggle.com/c/house-prices-advanced-regression-techniques .  I want you to do the following.

```{r}
#download the train data
train <- read.csv("https://raw.githubusercontent.com/Riteshlohiya/Data605_Final_Project/master/train.csv", header = TRUE, stringsAsFactors = FALSE)
```

##Descriptive and Inferential Statistics.

Provide univariate descriptive statistics and appropriate plots for the training data set.  Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. Derive a correlation matrix for any THREE quantitative variables in the dataset.  Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide a 80% confidence interval.  Discuss the meaning of your analysis.  Would you be worried about familywise error? Why or why not?

Subsetting the dataset to get only the numeric columns.


First i will plot scatterplots for Gross living area and Sale price.

```{r}
ggplot(train, aes(x=GrLivArea, y=SalePrice)) + geom_jitter(color='seagreen4') + theme_classic() + 
labs(title ='Scatter Plot of Gross living area vs Sale price') + theme(plot.title = element_text(hjust = 5000)) 
```

Scatter plot for Masonry veneer area in square feet and  scale price

```{r}
ggplot(train, aes(x=MasVnrArea, y=SalePrice)) + geom_jitter(color='seagreen4') + theme_classic() + 
labs(title ='Scatter Plot of Masonry veneer area vs Sale price') + theme(plot.title = element_text(hjust = 5000)) 
```

From the above scatter plots we can see that there is some kind of relation between the independent and dependent variables.

Lets now so some descriptive analysis on the available data.

## Descriptive statistics and plots 

```{r}
num <- unlist(lapply(train, is.numeric))
train_num <- train[, num]
summary(train_num)
```

#Bar plots to check relation to Sale price.



```{r, fig.width = 18, fig.height = 14, echo = FALSE}

t1 <- ggplot(train, aes(x = MSZoning)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) +
scale_y_continuous(labels=percent) +
ggtitle("Zoning Proportions") + theme(text = element_text(size = 15))
train$MSZoning <- factor(train$MSZoning)

t2 <- ggplot(train, aes(x = Street)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Street Proportions") + theme(text = element_text(size = 15))
train$Street <- factor(train$Street) 

t3 <- ggplot(train, aes(x = Alley)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Alley Proportions")+ theme(text = element_text(size = 15))
train$Alley <- factor(train$Alley)

t4 <- ggplot(train, aes(x = LotShape)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("LotShape Proportions")+ theme(text = element_text(size = 15))
train$LotShape <- factor(train$LotShape)

t5 <- ggplot(train, aes(x = LandContour)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("LandContour Proportions")+ theme(text = element_text(size = 15))
train$LandContour <- factor(train$LandContour)

t6 <- ggplot(train, aes(x = Utilities)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Utilities Proportions")+ theme(text = element_text(size = 15))
train$Utilities <- factor(train$Utilities)

t6 <- ggplot(train, aes(x = LotConfig)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("LotConfig Proportions")+ theme(text = element_text(size = 15))
train$LotConfig <- factor(train$LotConfig)

t7 <- ggplot(train, aes(x = LandSlope)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("LandSlope Proportions")+ theme(text = element_text(size = 15))
train$LandSlope <- factor(train$LandSlope)

t8 <- ggplot(train, aes(x = Neighborhood)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Neighborhood Proportions")+ theme(text = element_text(size = 15))
train$Neighborhood <- factor(train$Neighborhood)

t9 <- ggplot(train, aes(x = Condition1)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Condition1 Proportions")+ theme(text = element_text(size = 15))
train$Condition1 <- factor(train$Condition1)

t10 <- ggplot(train, aes(x = Condition2)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Condition2 Proportions")+ theme(text = element_text(size = 15))
train$Condition2 <- factor(train$Condition2)

t11 <- ggplot(train, aes(x = BldgType)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("BldgType Proportions")+ theme(text = element_text(size = 15))
train$BldgType <- factor(train$BldgType)

t12 <- ggplot(train, aes(x = HouseStyle)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("HouseStyle Proportions")+ theme(text = element_text(size = 15))
train$HouseStyle <- factor(train$HouseStyle)

t13 <- ggplot(train, aes(x = RoofStyle)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("RoofStyle Proportions")+ theme(text = element_text(size = 15))
train$RoofStyle <- factor(train$RoofStyle)

t14 <- ggplot(train, aes(x = RoofMatl)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("RoofMatl Proportions")+ theme(text = element_text(size = 15))
train$RoofMatl <- factor(train$RoofMatl)

t15 <- ggplot(train, aes(x = Exterior1st)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Exterior1st Proportions")+ theme(text = element_text(size = 15))
train$Exterior1st <- factor(train$Exterior1st)

t16 <- ggplot(train, aes(x = Exterior2nd)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Exterior2nd Proportions")+ theme(text = element_text(size = 15))
train$Exterior2nd <- factor(train$Exterior2nd)

t17 <- ggplot(train, aes(x = MasVnrType)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("MasVnrType Proportions")+ theme(text = element_text(size = 15))
train$MasVnrType <- factor(train$MasVnrType)

t18 <- ggplot(train, aes(x = ExterQual)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("ExterQual Proportions")+ theme(text = element_text(size = 15))
train$ExterQual <- factor(train$ExterQual)

t19 <- ggplot(train, aes(x = ExterCond)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("ExterCond Proportions")+ theme(text = element_text(size = 15))
train$ExterCond <- factor(train$ExterCond)

t20 <- ggplot(train, aes(x = Foundation)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Foundation Proportions")+ theme(text = element_text(size = 15))
train$Foundation <- factor(train$Foundation)

t21 <- ggplot(train, aes(x = BsmtQual)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("BsmtQual Proportions")+ theme(text = element_text(size = 15))
train$BsmtQual <- factor(train$BsmtQual)

t22 <- ggplot(train, aes(x = BsmtCond)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("BsmtCond Proportions")+ theme(text = element_text(size = 15))
train$BsmtCond <- factor(train$BsmtCond)

t23 <- ggplot(train, aes(x = BsmtExposure)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("BsmtExposure Proportions")+ theme(text = element_text(size = 15))
train$BsmtExposure <- factor(train$BsmtExposure)

t24 <- ggplot(train, aes(x = BsmtFinType1)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("BsmtFinType1 Proportions")+ theme(text = element_text(size = 15))
train$BsmtFinType1 <- factor(train$BsmtFinType1)

t25 <- ggplot(train, aes(x = BsmtFinType2)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("BsmtFinType2 Proportions")+ theme(text = element_text(size = 15))
train$BsmtFinType2 <- factor(train$BsmtFinType2)

t26 <- ggplot(train, aes(x = Heating)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Heating Proportions")+ theme(text = element_text(size = 15))
train$Heating <- factor(train$Heating)

t27 <- ggplot(train, aes(x = HeatingQC)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("HeatingQC Proportions")+ theme(text = element_text(size = 15))
train$HeatingQC <- factor(train$HeatingQC)

t28 <- ggplot(train, aes(x = CentralAir)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("CentralAir Proportions")+ theme(text = element_text(size = 15))
train$CentralAir <- factor(train$CentralAir)

t29 <- ggplot(train, aes(x = Electrical)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Electrical Proportions")+ theme(text = element_text(size = 15))
train$Electrical <- factor(train$Electrical)

t30 <- ggplot(train, aes(x = KitchenQual)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("KitchenQual Proportions")+ theme(text = element_text(size = 15))
train$KitchenQual <- factor(train$KitchenQual)

t31 <- ggplot(train, aes(x = Functional)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Functional Proportions")+ theme(text = element_text(size = 15))
train$Functional <- factor(train$Functional)

t32 <- ggplot(train, aes(x = FireplaceQu)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("FireplaceQu Proportions")+ theme(text = element_text(size = 15))
train$FireplaceQu <- factor(train$FireplaceQu)

t33 <- ggplot(train, aes(x = GarageType)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("GarageType Proportions")+ theme(text = element_text(size = 15))
train$GarageType <- factor(train$GarageType)

t34 <- ggplot(train, aes(x = GarageFinish)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("GarageFinish Proportions")+ theme(text = element_text(size = 15))
train$GarageFinish <- factor(train$GarageFinish)

t35 <- ggplot(train, aes(x = GarageQual)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("GarageQual Proportions")+ theme(text = element_text(size = 15))
train$GarageQual <- factor(train$GarageQual)

t36 <- ggplot(train, aes(x = GarageCond)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("GarageCond Proportions")+ theme(text = element_text(size = 15))
train$GarageCond <- factor(train$GarageCond)

t37 <- ggplot(train, aes(x = PavedDrive)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("PavedDrive Proportions")+ theme(text = element_text(size = 15))
train$PavedDrive <- factor(train$PavedDrive)

t38 <- ggplot(train, aes(x = PoolQC)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("PoolQC Proportions")+ theme(text = element_text(size = 15))
train$PoolQC <- factor(train$PoolQC)

t39 <- ggplot(train, aes(x = Fence)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("Fence Proportions")+ theme(text = element_text(size = 15))
train$Fence <- factor(train$Fence)

t40 <- ggplot(train, aes(x = MiscFeature)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("MiscFeature Proportions")+ theme(text = element_text(size = 15))
train$MiscFeature <- factor(train$MiscFeature)

t41 <- ggplot(train, aes(x = SaleType)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("SaleType Proportions")+ theme(text = element_text(size = 15))
train$SaleType <- factor(train$SaleType)

t42 <- ggplot(train, aes(x = SaleCondition)) +  
geom_bar(aes(y = (..count..)/sum(..count..))) + 
scale_y_continuous(labels=percent) +
ggtitle("SaleCondition Proportions")+ theme(text = element_text(size = 15))
train$SaleCondition <- factor(train$SaleCondition)

grid.arrange(
t1, t2, t3, t4, t5, t6, t7, t8, t9
)

grid.arrange(
t10, t11, t12, t13, t14, t15, t16, t17, t18
)

grid.arrange(
t19, t20, t21, t22, t23, t24, t25, t26, t27
)

grid.arrange(
t28, t29, t30, t31, t31, t33, t34, t35, t36
)

grid.arrange(
t36, t38, t39, t40, t41, t42
)




```



## Histograms for Ordinal Data 



```{r, fig.width = 9, fig.height = 7, echo = FALSE}
par(mfrow = c(5,3), oma = c(1, 1, 0, 0), mar=c(2, 3, 0, 1) + 2)
hist(train$MSSubClass, breaks = 100, col = 'blue')
hist(train$LotFrontage, breaks = 100, col = 'blue')
hist(train$LotArea, breaks = 100, col = 'blue')
hist(train$OverallQual, breaks = 100, col = 'blue')
hist(train$OverallCond, breaks = 100, col = 'blue')
hist(train$YearBuilt, breaks = 100, col = 'blue')
hist(train$YearRemodAdd, breaks = 100, col = 'blue')
hist(train$MasVnrArea, breaks = 100, col = 'blue')
hist(train$BsmtFinSF1, breaks = 100, col = 'blue')
hist(train$BsmtFinSF2, breaks = 100, col = 'blue')
hist(train$BsmtUnfSF, breaks = 100, col = 'blue')
hist(train$TotalBsmtSF, breaks = 100, col = 'blue')
hist(train$X1stFlrSF, breaks = 100, col = 'blue')
hist(train$X2ndFlrSF, breaks = 100, col = 'blue')
hist(train$LowQualFinSF, breaks = 100, col = 'blue')
hist(train$GrLivArea, breaks = 100, col = 'blue')
hist(train$BsmtFullBath, breaks = 100, col = 'blue')
hist(train$BsmtHalfBath, breaks = 100, col = 'blue')
hist(train$FullBath, breaks = 100, col = 'blue')
hist(train$HalfBath, breaks = 100, col = 'blue')
hist(train$BedroomAbvGr, breaks = 100, col = 'blue')
hist(train$KitchenAbvGr, breaks = 100, col = 'blue')
hist(train$TotRmsAbvGrd, breaks = 100, col = 'blue')
hist(train$Fireplaces, breaks = 100, col = 'blue')
hist(train$GarageYrBlt, breaks = 100, col = 'blue')
hist(train$GarageCars, breaks = 100, col = 'blue')
hist(train$GarageArea, breaks = 100, col = 'blue')
hist(train$WoodDeckSF, breaks = 100, col = 'blue')
hist(train$OpenPorchSF, breaks = 100, col = 'blue')
hist(train$EnclosedPorch, breaks = 100, col = 'blue')
hist(train$X3SsnPorch, breaks = 100, col = 'blue')
hist(train$ScreenPorch, breaks = 100, col = 'blue')
hist(train$PoolArea, breaks = 100, col = 'blue')
hist(train$MiscVal, breaks = 100, col = 'blue')
hist(train$MoSold, breaks = 100, col = 'blue')
hist(train$YrSold, breaks = 100, col = 'blue')
hist(train$SalePrice, breaks = 100, col = 'blue')
hist(train$MSSubClass, breaks = 100, col = 'blue')
hist(train$LotFrontage, breaks = 100, col = 'blue')
hist(train$LotArea, breaks = 100, col = 'blue')
hist(train$OverallQual, breaks = 100, col = 'blue')
hist(train$OverallCond, breaks = 100, col = 'blue')
hist(train$YearBuilt, breaks = 100, col = 'blue')
hist(train$YearRemodAdd, breaks = 100, col = 'blue')
hist(train$MasVnrArea, breaks = 100, col = 'blue')
hist(train$BsmtFinSF1, breaks = 100, col = 'blue')
hist(train$BsmtFinSF2, breaks = 100, col = 'blue')
hist(train$BsmtUnfSF, breaks = 100, col = 'blue')
hist(train$TotalBsmtSF, breaks = 100, col = 'blue')
hist(train$X1stFlrSF, breaks = 100, col = 'blue')
hist(train$X2ndFlrSF, breaks = 100, col = 'blue')
hist(train$LowQualFinSF, breaks = 100, col = 'blue')
hist(train$GrLivArea, breaks = 100, col = 'blue')
hist(train$BsmtFullBath, breaks = 100, col = 'blue')
hist(train$BsmtHalfBath, breaks = 100, col = 'blue')
hist(train$FullBath, breaks = 100, col = 'blue')
hist(train$HalfBath, breaks = 100, col = 'blue')
hist(train$BedroomAbvGr, breaks = 100, col = 'blue')
hist(train$KitchenAbvGr, breaks = 100, col = 'blue')
hist(train$TotRmsAbvGrd, breaks = 100, col = 'blue')
hist(train$Fireplaces, breaks = 100, col = 'blue')
hist(train$GarageYrBlt, breaks = 100, col = 'blue')
hist(train$GarageCars, breaks = 100, col = 'blue')
hist(train$GarageArea, breaks = 100, col = 'blue')
hist(train$WoodDeckSF, breaks = 100, col = 'blue')
hist(train$OpenPorchSF, breaks = 100, col = 'blue')
hist(train$EnclosedPorch, breaks = 100, col = 'blue')
hist(train$X3SsnPorch, breaks = 100, col = 'blue')
hist(train$ScreenPorch, breaks = 100, col = 'blue')
hist(train$PoolArea, breaks = 100, col = 'blue')
hist(train$MiscVal, breaks = 100, col = 'blue')
hist(train$MoSold, breaks = 100, col = 'blue')
hist(train$YrSold, breaks = 100, col = 'blue')
hist(train$SalePrice, breaks = 100, col = 'blue')
```

##Derive a correlation matrix for any THREE quantitative variables in the dataset

Selected variables are: SalePrice,TotalBsmtSF,GrLivArea

```{r}
train_cor <- train[c("SalePrice", "TotalBsmtSF", "GrLivArea")]
train_cor_matrix <- cor(train_cor, use = "complete.obs")
train_cor_matrix
```

The Matrix suggests that there are strong to moderate corelation exists between these three variables. 'Saleprice' has strong corelations with 'TotalBsmtSF' and 'GrLivArea' with corelation coefficients of .61 and .708 respectively while 'TotalBsmtSF' and 'GrLivArea' have moderate corelation between them with coefficient of .45.

```{r}
pairs.panels(train_cor_matrix)
```

##Corelation test bwteen each pair:

##Test between 'TotalBsmtSF' and 'SalePrice'

```{r}
cor.test(train$TotalBsmtSF, train$SalePrice, method = "pearson", conf.level = 0.80)
```

##Test between 'GrLivArea' and 'SalePrice'

```{r}
cor.test(train$GrLivArea, train$SalePrice, method = "pearson", conf.level = 0.80)
```

##Test between 'GrLivArea' and 'TotalBsmtSF'

```{r}
cor.test(train$GrLivArea, train$TotalBsmtSF, method = "pearson", conf.level = 0.80)
```

Since all three p-values are less than .05, the variables are significantly correlated.

##Would you be worried about familywise error? Why or why not?

Yes, there are variables in this dataset that might have impact on the corelation of the the pairs of selected variables that are being tested here. There is a scope for familywise error which might cause rejecting of true Null hypothesis.

#Linear Algebra and Correlation:

Invert your 3 x 3 correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal.) Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. Conduct LU decomposition on the matrix.  

##Correlation Matrix:

```{r}
train_cor_matrix
```

##Precision matrix:

```{r}
pre_matrix <- solve(train_cor_matrix)
pre_matrix
```

##Multiplication of correlation matrix by the precision matrix:

```{r}
round((train_cor_matrix %*% pre_matrix), 2)
```

##Multiplication of precision matrix by the correlation matrix:

```{r}
round((pre_matrix %*% train_cor_matrix), 2)
```

Both the matrix is identical.

##LU decomposition of corelation matrix:

```{r}
cor_lu <- lu(train_cor_matrix)
cor_lu_exd <- expand(cor_lu)

L_cor <- cor_lu_exd$L
U_cor <- cor_lu_exd$U

L_cor
U_cor
```

##LU decomposition of precision matrix:


```{r}
pre_lu <- lu(pre_matrix)
pre_lu_exd <- expand(pre_lu)

L_pre <- pre_lu_exd$L
U_pre <- pre_lu_exd$U

L_pre
U_pre
```

Lets multiply the lower and uper matrix and see if it returns the original matrices or not.

```{r}
L_cor %*% U_cor
L_pre %*% U_pre
```

It returns the original matrices.

#Calculus-Based Probability & Statistics

Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that  is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.  Then load the MASS package and run fitdistr to fit an exponential probability density function.  (See  https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/fitdistr.html ).  Find the optimal value of ??? for this distribution, and then take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, ???)).  Plot a histogram and compare it with a histogram of your original variable.   Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).   Also generate a 95% confidence interval from the empirical data, assuming normality.  Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.

```{r}
min(train$GrLivArea)
```

The minimum value is 334, so i dont think we need shifting.

Fitting the exponential probability density function:

```{r}
expo <- fitdistr(train$GrLivArea, densfun = "exponential")
options(scipen = 999)
expo$estimate
```

```{r}
smpl <- rexp(1000, expo$estimate)
```

##Histogram of the samples and the original :

```{r}
hist(train$GrLivArea)
hist(smpl)
```

The samples data is more skewed than the original data.

Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF):

```{r}
P = ecdf(smpl)
plot(P)

c <- quantile(P, c(0.05, 0.95))
c
```

The 5th and 95th percentiles of the samples (simulated data) are 87.16779 and 4759.05707 respectively.

Generate a 95% confidence interval from the empirical data, assuming normality:

```{r}
er <- qnorm(0.975) * sd(train$GrLivArea)/sqrt(length(train$GrLivArea))
conf_95 <- c(mean(train$GrLivArea) - er, mean(train$GrLivArea) + er)
conf_95
```

The 95% confidence interval is 1488.509 1542.418

Provide the empirical 5th percentile and 95th percentile of the data:

```{r}
e <- quantile(X, c(0.05, 0.95))
e
```

The 5th and 95th percentiles are 4.070 and 21.735 respectively.

#Modeling:


##Data preparation

```{r}
#download the train data
test <- read.csv("https://raw.githubusercontent.com/Riteshlohiya/Data605_Final_Project/master/test.csv", header = TRUE, stringsAsFactors = FALSE)
 
train <- cbind.data.frame(train, RecType = "Train")
test <- cbind.data.frame(test, RecType = "Test")

train_test <- rbind.data.frame(train, test, stringsAsFactors = FALSE)
```


We will see the missing values in the dataset. For this i have used Amelia package

##Missing value plot before treatment:

```{r}
missmap(train_test, main = "Missing values vs observed",  color='dodgerblue')
```



We can see there are lots of missing values. We will replace the missing values.

```{r}

#Missing value handling

train_test[sapply(train_test, is.factor)] <- lapply(train_test[sapply(train_test, is.factor)], as.character)

train_test$GarageYrBlt[is.na(train_test$GarageYrBlt)] <- train_test$YearBuilt[is.na(train_test$GarageYrBlt)] 
train_test$LotFrontage[is.na(train_test$LotFrontage)] <- 0
train_test$MasVnrArea[is.na(train_test$MasVnrArea)] <- 0
train_test$Alley[is.na(train_test$Alley)] <- 'None'
train_test$Utilities[is.na(train_test$Utilities)] <- 'NoSeWa'
train_test$MasVnrType[is.na(train_test$MasVnrType)] <- 'None'
train_test$BsmtQual[is.na(train_test$BsmtQual)] <- 'None'
train_test$BsmtCond[is.na(train_test$BsmtCond)] <- 'Xa'
train_test$BsmtExposure[is.na(train_test$BsmtExposure)] <- 'Xb'
train_test$BsmtFinType1[is.na(train_test$BsmtFinType1)] <- 'Xc'
train_test$BsmtFinType2[is.na(train_test$BsmtFinType2)] <- 'Xd'
train_test$GarageType [is.na(train_test$GarageType )] <- 'Xe'
train_test$GarageFinish[is.na(train_test$GarageFinish)] <- 'Xf'
train_test$GarageQual[is.na(train_test$GarageQual)] <- 'Xg'
train_test$GarageCond[is.na(train_test$GarageCond)] <- 'Xh'
train_test$Electrical[is.na(train_test$Electrical)] <- 'None'
train_test$FireplaceQu[is.na(train_test$FireplaceQu)] <- 'None'
train_test$PoolQC[is.na(train_test$PoolQC)] <- 'None'
train_test$Fence[is.na(train_test$Fence)] <- 'None'
train_test$MiscFeature[is.na(train_test$MiscFeature)] <- 'None'
train_test$MSZoning[is.na(train_test$MSZoning)] <- 'C (all)'
train_test$Utilities[is.na(train_test$Utilities)] <- 'AllPub'
train_test$BsmtFullBath[is.na(train_test$BsmtFullBath)] <- 0
train_test$BsmtHalfBath[is.na(train_test$BsmtHalfBath)] <- 0
train_test$Exterior1st[is.na(train_test$Exterior1st)] <- 'BrkFace'
train_test$Exterior2nd[is.na(train_test$Exterior2nd)] <- 'BrkFace'
train_test$Functional[is.na(train_test$Functional)] <- 'Typ'
train_test$BsmtFinSF1[is.na(train_test$BsmtFinSF1)] <- 0
train_test$BsmtFinSF2[is.na(train_test$BsmtFinSF2)] <- 0
train_test$BsmtUnfSF[is.na(train_test$BsmtUnfSF)] <- 0
train_test$TotalBsmtSF[is.na(train_test$TotalBsmtSF)] <- 0
train_test$GarageCars[is.na(train_test$GarageCars)] <- 0
train_test$GarageArea[is.na(train_test$GarageArea)] <- 0
train_test$SaleType[is.na(train_test$SaleType)] <- 'None'
train_test$SalePrice[is.na(train$SalePrice)] <- 0


train_test[sapply(train_test, is.character)] <- lapply(train_test[sapply(train_test, is.character)], as.factor)
```


##Missing value plot after treatment:

```{r}
missmap(train_test, main = "Missing values vs observed",  color='dodgerblue')
```

Creating dummy values:

```{r}
train_test <- createDummyFeatures(train_test, method = "reference")
```

Outlier treatment using mean:

```{r}
train_test$TotalBsmtSF[train_test$TotalBsmtSF > 6000] <- mean(train_test$TotalBsmtSF[train_test$TotalBsmtSF < 6000])
train_test$X1stFlrSF[train_test$X1stFlrSF > 4000] <- mean(train_test$X1stFlrSF[train_test$X1stFlrSF < 4000])
``` 

```{r}
# Split the combined transformed dataset into train and test.
data_train <- train_test[train_test$Train == 1, ]
data_test <- train_test[train_test$Train == 0, ]

# remove id and train/test flag variables from train dataset
data_train <- subset(data_train, select = -c(Id,Train))

# remove SalePrice variables from test dataset
data_test <- subset(data_test, select = -c(SalePrice,Train))
```



##Model 1: Taking all the variables: 


```{r}
model1 <- lm(SalePrice ~ ., data = data_train)
summary(model1)
```

R-squared is 0.93, This means 93% variance of the sale price can be explained by predictor variables in the model. F-statistic is 66.94 with 1207 of degree of freedom and p-value is also very small.

##Model 2: Taking only the variables whoes P <.05.

```{r}
model2 <- lm(SalePrice ~ LotArea + OverallQual + OverallCond + YearBuilt + MasVnrArea + BsmtFinSF1 + BsmtFinSF2 + BsmtUnfSF  + TotalBsmtSF +
                X1stFlrSF + X2ndFlrSF + BedroomAbvGr + KitchenAbvGr + Fireplaces + GarageArea  + ScreenPorch + PoolArea +
               MSZoning.FV + MSZoning.RL  + Pave + LotConfig.CulDSac  + LandSlope.Sev + Neighborhood.Edwards +
               Neighborhood.Mitchel + Neighborhood.NAmes + Neighborhood.NoRidge + Neighborhood.NridgHt + Neighborhood.NWAmes + 
               Neighborhood.StoneBr + Condition1.Norm + Condition2.PosN + Condition2.RRAe + RoofStyle.Shed +
               RoofMatl.CompShg  + RoofMatl.Tar.Grv +  RoofMatl.WdShake + ExterQual.Gd + ExterQual.TA + BsmtQual.Gd + BsmtQual.TA +
               BsmtExposure.Gd + BsmtExposure.No  + KitchenQual.Fa + KitchenQual.Gd + KitchenQual.TA + 
               Functional.Typ  + GarageQual.Fa +  GarageQual.Gd + GarageQual.Po + GarageQual.TA + GarageCond.Fa +
               GarageCond.Gd + GarageCond.Po + GarageCond.TA + PoolQC.Fa + PoolQC.Gd + PoolQC.None, data = data_train)
summary(model2)
```


This model did not give any performance improvement. R-squared is 0.91, This means 91% variance of the sale price can be explained by predictor variables in the model. F-statistic is 249.2 with 1402 of degree of freedom and p-value remains the same.

#Prediction:


```{r}
model1_data <- data_test
model2_data <- data_test
# modelColumns <- colnames(HouseDF) testDF_model <-
# testData[,colnames(testData) %in% modelColumns]

model1_data$salePrice <- predict(model1, data_test)
model2_data$salePrice <- predict(model2, data_test)

Id <- data_test$Id
# Kaggle dataset for model1
salePrice <- model1_data$salePrice
kaggleData1 <- data.frame(cbind(Id, salePrice))
kaggleData1[is.na(kaggleData1)] <- 0
# write.csv(kaggleData_modelDF,'kaggleData_model.csv')

# Kaggle dataset for model2
salePrice <- model2_data$salePrice
kaggleData2 <- data.frame(cbind(Id, salePrice))
kaggleData2[is.na(kaggleData2)] <- 0

write.csv(kaggleData1,'C:/Users/rites/Documents/GitHub/Data605_Final_Project/kaggleout1.csv', row.names = F)
write.csv(kaggleData2,'C:/Users/rites/Documents/GitHub/Data605_Final_Project/kaggleout2.csv', row.names = F)

```

#Kaggle.com user name : riteshlohiya999
#Kaggle score: 
#Model1 : 0.47336
#Model2: 0.50297

For my 1st model: R-squared is 0.93, This means 93% variance of the sale price can be explained by predictor variables in the model. F-statistic is 66.94 with 1207 of degree of freedom and p-value is also very small. Kaggke score is 0.47336

For my 2nd model: This model did not give any performance improvement. R-squared is 0.91, This means 91% variance of the sale price can be explained by predictor variables in the model. F-statistic is 249.2 with 1402 of degree of freedom and p-value remains the same.












